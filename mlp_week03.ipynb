{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QNnuZONqYsb4"
   },
   "source": [
    "# Week 3: Clustering\n",
    "\n",
    "\n",
    "In this workshop, we will work through a set of problems clustering, another cannonical form of unsupervised learning. Clustering is an important tool that is used to discover homogeneous groups of data points within a heterogeneous population. It can be the main goal in some problems, while in others it may be used in EDA to understand the main types of behavior in the data or in feature engineering.   \n",
    "\n",
    "We will start by generating some artificial data, and then we will utilize clustering algorithms described in lectures and explore the impact of feature engineering on the solution. We will then attempt to find clusters in a gene expression dataset. \n",
    "\n",
    "As usual, the worksheets will be completed in teams of 2-3, using **pair programming**, and we have provided cues to switch roles between driver and navigator. When completing worksheets:\n",
    "\n",
    ">- You will have tasks tagged by (CORE) and (EXTRA). \n",
    ">- Your primary aim is to complete the (CORE) components during the WS session, afterwards you can try to complete the (EXTRA) tasks for your self-learning process. \n",
    ">- Look for the üèÅ as cue to switch roles between driver and navigator.\n",
    "\n",
    "Instructions for submitting your workshops can be found at the end of worksheet. As a reminder, you must submit a pdf of your notebook on Learn by 16:00 PM on the Friday of the week the workshop was given. \n",
    "\n",
    "As you work through the problems it will help to refer to your lecture notes (navigator). The exercises here are designed to reinforce the topics covered this week. Please discuss with the tutors if you get stuck, even early on! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outline\n",
    "\n",
    "1. [Problem Definition and Setup: Simulated Example](#setup1)\n",
    "2. [K-means: Simulated Example](#kmeans1)\n",
    "3. [Hierarchical Clustering: Simulated Example](#hc)\n",
    "4. [Gene Expression Data](#genedata)\n",
    "5. [Hierarchical Clustering: Gene Expression Data](#hc_genedata)\n",
    "6. [K-means Clustering: Gene Expression Data](#kmeans_genedata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YAdywZYoZa5T"
   },
   "source": [
    "# Problem Definition and Setup: Simulated Example <a id='setup1'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packages\n",
    "\n",
    "First, lets load in some packages to get us started. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "am3xGa8BYohT"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import seaborn as sns \n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from scipy.cluster import hierarchy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dWAWprSdZ11V"
   },
   "source": [
    "## Data: Simulated Example\n",
    "\n",
    "We will begin with a simple simulated example in which there are truly three clusters. We assume that there are $D=2$ features and within each cluster, the data points are generated from a spherical normal distribution $N(\\mathbf{m}_k, \\sigma^2_k \\mathbf{I})$ for clusters $k=1,2,3$, where both the mean $\\mathbf{m}_k$ and variance $\\sigma^2_k$ are different across clusters. Specifically, we assume that: \n",
    "\n",
    "* Cluster 1: contains $|C_1|=500$ points with mean vector $\\mathbf{m}_1 = \\begin{pmatrix} 0 \\\\ 4 \\end{pmatrix}$ with standard deviation $\\sigma_1 = 2$.\n",
    "* Cluster 2: contains $|C_2|=250$ points with mean vector $\\mathbf{m}_2 = \\begin{pmatrix} 0 \\\\ -4 \\end{pmatrix}$ with standard deviation $\\sigma_2 = 1$.\n",
    "* Cluster 3: contains $|C_3|=100$ points with mean vector $\\mathbf{m}_3 = \\begin{pmatrix} -4 \\\\ 0 \\end{pmatrix}$ with standard deviation $\\sigma_3 = 0.5$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following code to generate the dataset described above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "id": "BzWG5w4SZv8q"
   },
   "outputs": [],
   "source": [
    "# Number of features\n",
    "D = 2\n",
    "\n",
    "# Cluster sizes\n",
    "N_1 = 500\n",
    "N_2 = 250\n",
    "N_3 = 100\n",
    "\n",
    "# Cluster means\n",
    "m_1 = np.array([0., 4.])\n",
    "m_2 = np.array([0., -4.])\n",
    "m_3 = np.array([-4., 0.])\n",
    "\n",
    "# Cluster standard deviations\n",
    "sd_1 = 2.\n",
    "sd_2 = 1.\n",
    "sd_3 = 0.5\n",
    "\n",
    "# Generate the data\n",
    "rnd = np.random.RandomState(5)\n",
    "X_1 = rnd.normal(loc = m_1, scale = sd_1, size = (N_1,D))\n",
    "X_2 = rnd.normal(loc = m_2, scale = sd_2, size = (N_2,D))\n",
    "X_3 = rnd.normal(loc = m_3, scale = sd_3, size = (N_3,D))\n",
    "X = np.vstack((X_1, X_2, X_3))\n",
    "\n",
    "# Save true cluster labels\n",
    "cl = np.hstack((np.repeat(1,N_1),np.repeat(2,N_2),np.repeat(3,N_3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that the size is correct\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üö© Exercise 1 (CORE)\n",
    "\n",
    "Visualise the data and color by the true cluster labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 609
    },
    "id": "mKM3uECSblEP",
    "outputId": "5327b071-bad4-4362-e404-17e67da71d98"
   },
   "outputs": [],
   "source": [
    "# Plot the data and color by cluster membership\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-means Clustering: Simulated Example <a id='kmeans1'></a>\n",
    "\n",
    "To perform K-means clustering, we will use `KMeans()` in `sklearn.cluster`. Documentation is available [here](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html), and for an overview of clustering methods available in `sklearn`, see [link](https://scikit-learn.org/stable/modules/clustering.html). There are different inputs we can specify when calling `KMeans()` such as:\n",
    "\n",
    "- `n_clusters`: the number of clusters. \n",
    "- `init`: which specifies the intialization of the centroids, e.g. can be set to `k-means++` for K-means++ initialization or `random` for random initialization.\n",
    "- `n_init`: which specifies the number of times the algorithm is run with different random initializations\n",
    "- `random_state`: this can bet set to a fixed number to make results reproducible.\n",
    "\n",
    "We can then use the `.fit()` method of `KMeans` to run the K-means algorithm on our data.\n",
    "\n",
    "After fitting, some of the relevant attributes of interest include:\n",
    "\n",
    "- `labels_`: cluster assignments of the data points.\n",
    "- `cluster_centers_`: mean corresponding to each cluster, stored in a matrix of size: number of clusters $K$ times number features $D$.\n",
    "- `inertia_`: the total within-cluster variation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üö© Exercise 2 (CORE)\n",
    "\n",
    "Let's start by exploring how the clustering changes across the K-means iterations. To do, set:\n",
    "\n",
    "- number of clusters to 3\n",
    "- initialization to random\n",
    "- number of times the algorithm is run to 1\n",
    "- fix the random seed to a number of your choice (e.g. 0)\n",
    "\n",
    "\n",
    "a) Now, fit the K-means algorithms with different values of the maximum number of iterations fixed to 1,2,3, and the default value of 300. \n",
    "\n",
    "b) Plot the clustering solution for the four different cases and comment on how it changes. \n",
    "\n",
    "c) How many iterations are needed for the convergence?\n",
    "<br><br>\n",
    "<details><summary><b><u>Hint</b></u></summary>\n",
    "    \n",
    "- To find the number of iterations, check the attributes of [`KMeans`](https://scikit-learn.org/1.5/modules/generated/sklearn.cluster.KMeans.html)\n",
    "    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part a\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üö© Exercise 3 (CORE)\n",
    "\n",
    "Next, compare the random intialization with K-means++ (in this case fix the number of different initializations to 10). Plot both clustering solutions. Which requires fewer iterations? and which provides a lower within-cluster variation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üö© Exercise 4 (CORE)\n",
    "\n",
    "Find the clustering solution using a different number of initializations equal to 1, 2, 5, 10, and 20. Visualize the results and print the within-cluster variation. Based on the results, how many intializations are needed? Try changing the random state; how does that change your conclusions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üèÅ **Now, is a good point to switch driver and navigator**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üö© Exercise 5 (CORE)\n",
    "\n",
    "Since we simulated the data, we know the true number of clusters. However, in practice this number is rarely known. Find the K-means solution with different choices of $K$ and plot the within-cluster variation as a function of $K$. Comment on the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üö© Exercise 6 (CORE)\n",
    "\n",
    "Now standardize the data and re-run the K-means algorithm. Qualitatively, how has standardising the data impacted performance? Can you argue why you observe what you see?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hierarchical Clustering: Simulated Example <a id='hc'></a>\n",
    "\n",
    "To perform hierarchical clustering, we will use the [`linkage()`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.cluster.hierarchy.linkage.html) function from `scipy.cluster.hierarchy`. The inputs to specify include\n",
    "\n",
    "-  the data. \n",
    "- `metric`: specifies the dissimarlity between data points. Defaults to the Euclidean distance.\n",
    "- `method`: specifies the type of linkage, e.g. complete, single, or average.\n",
    "\n",
    "Then, we can use [`dendrogram()`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.cluster.hierarchy.dendrogram.html) from `scipy.cluster.hierarchy` to plot the dendrogram.\n",
    "\n",
    "Note that you can also use [`AgglomerativeClustering`](https://scikit-learn.org/1.5/modules/generated/sklearn.cluster.AgglomerativeClustering.html) from `sklearn.cluster`, which similarly has options for `metric` to specify the distance and `linkage` to specify the type of linkage. However, `sklearn` does not have its own functions for plotting the dendogram and use must use the tools from `scipy.cluster.hierarchy`.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üö© Exercise 7 (CORE)\n",
    "\n",
    "a) Use hierarchical clustering with complete linkage and the Euclidean distance to cluster the simulated data. Name the object `hc_comp`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The name of the returned object from hierarchy.linkage should be hc_comp for part b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Plot the dendogram by running the code below. Try changing the 'color_threshold' to a number (e.g. 11) to color the branches of the tree below the threshold with different colors, thus, identifying the clusters if the tree were to be cut at that threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the dendrogram\n",
    "cargs = {'color_threshold': -np.inf,'above_threshold_color':'black'}\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10,6))\n",
    "hierarchy.dendrogram(hc_comp, ax=ax, **cargs, no_labels=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to change the color threshold to visualize clusters obtained by cutting the tree\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Now, use the function `cut_tree()` from `scipy.cluster.hierarchy` to determine the cluster labels associated with a given cut of the dendrogram. You can either specify the number of clusters via `n_clusters` or the height/threshold at which to cut via `height`. Plot the data colored by cluster membership."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cut the tree at a specified number of clusters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üö© Exercise 8 (CORE)\n",
    "\n",
    "Now try changing the linkage to single and average. Does this affect on the results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change to single linkage, plot the dendrogram, and visualize the clustering solution by cutting the tree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change to average linkage, plot the dendrogram, and visualize the clustering solution by cutting the tree\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üèÅ **Now, is a good point to switch driver and navigator**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tG5WyUN2GbiB"
   },
   "source": [
    "# Gene Expression Data <a id='genedata'></a>\n",
    "\n",
    "Now, we will consider a more complex real dataset with a larger feature space. \n",
    "\n",
    "The dataset is the **NCI cancer microarray dataset** discussed in both *Introduction to Statistical Learning* and  *Elements of Statistical Learning*. The dataset consists of $D=6830$ gene expression measurements for each of $N=64$ cancer cell lines. The aim is to determine whether there are groups among the cell lines with similar gene expression patterns. This is an example of a high-dimensional dataset with $D$ much larger than $N$, which makes visualization difficult. The $N=64$ cancer cell lines have been obtained from samples of cancerous tisses, corresponding to 14 different types of cancer. However, our focus remains unsupervised learning and we will use the cancer labels only to plot. \n",
    "\n",
    "We first need to read in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "id": "leJH2StnFPaI"
   },
   "outputs": [],
   "source": [
    "#Fetch the data and cancer labels\n",
    "url_data = 'https://web.stanford.edu/~hastie/ElemStatLearn/datasets/nci.data.csv'\n",
    "url_labels = 'https://web.stanford.edu/~hastie/ElemStatLearn/datasets/nci.label.txt'\n",
    "\n",
    "X = pd.read_csv(url_data)\n",
    "y = pd.read_csv(url_labels, header=None)\n",
    "\n",
    "# clean data and follow convention in the notes that features are columns:\n",
    "X = X.drop(labels='Unnamed: 0', axis=1).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H97lGxLsOa9v"
   },
   "source": [
    "Let's visualise the data with a contour plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 348
    },
    "id": "Tb1J3rLMNula",
    "outputId": "0eb88c8e-0415-4be5-f646-1b677105071c"
   },
   "outputs": [],
   "source": [
    "# Contour plot of the gene expression data\n",
    "fig = plt.figure(figsize=(30,5))\n",
    "ax = fig.add_subplot(111)\n",
    "contours = ax.contourf(X, cmap='inferno')#, vmax=4, vmin=-4)\n",
    "cbar = plt.colorbar(contours)\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n",
    "ax.set_xlabel(\"Gene expression\", fontsize=22)\n",
    "ax.set_ylabel(\"Tissue sample\", fontsize=22)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mozq7WKja2gW"
   },
   "source": [
    "We now convert our pandas dataframe into a numpy array and create integer labels for cancer type (for plotting purposes)\n",
    "\n",
    "If you visualise the labels, you will notice there are lots of inconsistencies with white space etc. Run the following code to clean the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the unique labels and counts\n",
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the labels by stripping the white space\n",
    "y_clean = np.asarray(y).flatten()\n",
    "for j in range(y_clean.size):\n",
    "    y_clean[j] = y_clean[j].strip()\n",
    "\n",
    "cancer_types = list(np.unique(y_clean))\n",
    "cancer_groups = np.array([cancer_types.index(lab) for lab in y_clean])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cancer_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_array = np.asarray(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "13qbb2jqbvhT"
   },
   "source": [
    "### üö© Exercise 9 (EXTRA)\n",
    "\n",
    "Perform a PCA of $\\mathbf X$ to visualize the data. Plot the first few principal component scores and color by cancer type. Do cell lines within the same cancer types seems to have similar scores? Make a scree plot of the proportion of variance explained. How many components does this suggest?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "id": "TEdWUy88bYMi"
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hierarchical Clustering: Gene Expression Data <a id='hc_genedata'></a>\n",
    "\n",
    "Now, let's perform hierarchical clustering on the gene expression data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üö© Exercise 10 (CORE)\n",
    "\n",
    "a) Plot the dendrogram with complete, single, and average linkage. Does the choice of linkage affect the results? Which linkage would you choose?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit hierarchical clustering with different types of linkage\n",
    "\n",
    "\n",
    "# Plot the dendogram\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Select a linkage and a number of clusters (by examining the dendrogram and jumps in the heights of the clusters merged). Plot the dendogram and color the branches to identify the clusters. Use the option `labels = np.asarray(y_clean), leaf_font_size=10` in `hierarchy.dendrogram` to add the cancer types as labels for each data point. Do you observe any patterns between the clusters and cancer types? You may also want to use `pd.crosstab` to compute a cross-tabulation to compare the clusters and cancer types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üèÅ **Now, is a good point to switch driver and navigator**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-means Clustering: Gene Expression Data <a id='kmeans_genedata'></a>\n",
    "\n",
    "Now, let's perform k-means clustering on the gene expression data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üö© Exercise 11 (CORE)\n",
    "\n",
    "Perform K-means clustering with the same number of clusters that you selected for hierarchical clustering. Are the results similar?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üö© Exercise 12 (EXTRA)\n",
    "\n",
    "Plot the two clustering solutions along with a plot of the data colored by the cancer types in the space spanned by the first two principal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "So3kpRjk2-yx"
   },
   "source": [
    "# Competing the Worksheet\n",
    "\n",
    "At this point you have hopefully been able to complete all the CORE exercises and attempted the EXTRA ones. Now \n",
    "is a good time to check the reproducibility of this document by restarting the notebook's\n",
    "kernel and rerunning all cells in order.\n",
    "\n",
    "Before generating the PDF, please go to Edit -> Edit Notebook Metadata and change 'Student 1' and 'Student 2' in the **name** attribute to include your name.\n",
    "\n",
    "Once that is done and you are happy with everything, you can then run the following cell \n",
    "to generate your PDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!jupyter nbconvert --to pdf mlp_week03.ipynb "
   ]
  }
 ],
 "metadata": {
  "authors": [
   {
    "name": "Student 1"
   },
   {
    "name": "Student 2"
   }
  ],
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "title": "MLPy Workshop 3"
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
